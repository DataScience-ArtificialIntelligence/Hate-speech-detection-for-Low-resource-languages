{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fZi-AUmPjIpG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Load the dataset from CSV with tab as the separator\n",
        "    df = pd.read_csv('SOLD.csv', sep='\\t')\n",
        "\n",
        "    # Convert labels to 0 and 1\n",
        "    df['label'] = df['label'].map({'NOT': 0, 'HOF': 1})\n",
        "\n",
        "    # Remove English text and special characters like @, #, !, ., etc.\n",
        "    df['text'] = df['text'].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|#|([A-Za-z0-9])|(\\.+)|\\t.+\", \"\", elem))\n",
        "\n",
        "    # Split the dataset into train and test sets (80% train, 20% test)\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Save the preprocessed train and test datasets to new CSV files\n",
        "    train_df.to_csv('train_dataset.csv', index=False)\n",
        "    test_df.to_csv('test_dataset.csv', index=False)\n",
        "\n",
        "    print(\"Preprocessing completed.\")\n",
        "    print(\"Train and test datasets saved to train_dataset.csv and test_dataset.csv\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMEgHnHzjMHI",
        "outputId": "b64dc609-2fbc-476b-e9d3-d68dd71ec6d0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing completed.\n",
            "Train and test datasets saved to train_dataset.csv and test_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "AfuLvClyjbWx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = train_df['text'].tolist()\n",
        "labels = train_df['label'].tolist()"
      ],
      "metadata": {
        "id": "hsDvz5KPjhG3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(texts), len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDagRy7sjj7Q",
        "outputId": "18958bab-ed64-4b47-d67f-5babf0cde128"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6000 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)"
      ],
      "metadata": {
        "id": "ipq5xwAxjnvF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)"
      ],
      "metadata": {
        "id": "JjLxrIR8jpsJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, AutoTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead, pipeline"
      ],
      "metadata": {
        "id": "1joFwV9Ijq9J"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"keshan/SinhalaBERTo\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"keshan/SinhalaBERTo\",num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMON6BuSjsrA",
        "outputId": "3e72c30f-c6ae-452c-d467-61c6f256a3ad"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at keshan/SinhalaBERTo and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model"
      ],
      "metadata": {
        "id": "FrWO85V2juyn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.data.iloc[index]['text']\n",
        "        label = self.data.iloc[index]['label']\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'label': torch.tensor(label)\n",
        "        }"
      ],
      "metadata": {
        "id": "_vXwL74NjwuF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(train_df)\n",
        "df['max_token_count'] = df['text'].apply(lambda x: len(tokenizer.encode(x)))\n",
        "\n",
        "# Find the maximum token count across all rows\n",
        "max_tokens = df['max_token_count'].max()\n",
        "\n",
        "print(\"Maximum token count:\", max_tokens)\n",
        "#=========================================================================================================\n",
        "\n",
        "dataset = Dataset(train_df, tokenizer, max_length= max_tokens)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the pre-trained model\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ETPvvkZjzQC",
        "outputId": "5df8698f-d854-4ee7-c7d0-6f1295acecd7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum token count: 392\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Define your CNN and LSTM model architecture\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, model, cnn_kernel_size, lstm_hidden_size, num_labels, dropout_rate):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "        self.bert = model\n",
        "        self.cnn = nn.Conv1d(in_channels=model.config.hidden_size, out_channels=64, kernel_size=cnn_kernel_size)\n",
        "        self.lstm = nn.LSTM(input_size=64, hidden_size=lstm_hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(lstm_hidden_size, num_labels)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        bert_hidden_states = outputs.last_hidden_state  # Use the last hidden state from BERT\n",
        "\n",
        "        # Permute dimensions for CNN\n",
        "        cnn_input = bert_hidden_states.permute(0, 2, 1)\n",
        "        cnn_output = self.cnn(cnn_input)\n",
        "\n",
        "        # Permute dimensions for LSTM\n",
        "        lstm_input = cnn_output.permute(0, 2, 1)\n",
        "        lstm_output, _ = self.lstm(lstm_input)\n",
        "\n",
        "        # Take the final hidden state from LSTM for classification\n",
        "        lstm_final_hidden = lstm_output[:, -1, :]\n",
        "\n",
        "        lstm_final_hidden = self.dropout(lstm_final_hidden)\n",
        "\n",
        "        logits = self.fc(lstm_final_hidden)\n",
        "        return logits\n",
        "\n",
        "# Set hyperparameters\n",
        "cnn_kernel_size = 3\n",
        "lstm_hidden_size = 128\n",
        "num_labels = 2\n",
        "dropout_rate = 0.5\n",
        "\n",
        "# Initialize your CNNLSTMModel\n",
        "modell = CNNLSTMModel(model, cnn_kernel_size, lstm_hidden_size, num_labels, dropout_rate)\n",
        "modell.to(device)\n",
        "\n",
        "# Set training parameters\n",
        "batch_size = 16\n",
        "learning_rate = 2e-5\n",
        "epochs = 1\n",
        "\n",
        "# Create data loader\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Create optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        logits = outputs.logits  # Use the logits attribute\n",
        "\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    print(f'Epoch {epoch + 1}/{epochs} - Loss: {avg_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "284py6kwj1KR",
        "outputId": "f2d7ca69-ea70-4852-dd71-f612ed358eb2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1 - Loss: 0.5401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(modell)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJI5CKYW2l7v",
        "outputId": "9a934203-77e2-4b9b-dc2e-e8d236b19257"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNLSTMModel(\n",
            "  (bert): RobertaForSequenceClassification(\n",
            "    (roberta): RobertaModel(\n",
            "      (embeddings): RobertaEmbeddings(\n",
            "        (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
            "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "        (token_type_embeddings): Embedding(1, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): RobertaEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0-5): 6 x RobertaLayer(\n",
            "            (attention): RobertaAttention(\n",
            "              (self): RobertaSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): RobertaSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): RobertaIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): RobertaOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (classifier): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (cnn): Conv1d(768, 64, kernel_size=(3,), stride=(1,))\n",
            "  (lstm): LSTM(64, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = \"/content/drive/MyDrive/sinhala_proj/ensemble\"\n",
        "model.save_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "fT1VqsM74Gyd"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/sinhala_proj/ensemble\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "tpuBvpDI4IDK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = test_df['text'].tolist()\n",
        "labels = test_df['label'].tolist()"
      ],
      "metadata": {
        "id": "WLYSMRsq2mUg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(texts), len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2yfDS873EYI",
        "outputId": "ec541a47-40e2-4948-a0ce-cbe0571ea650"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500 1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(test_df, model, tokenizer, batch_size=16, device='cuda'):\n",
        "    predicted_labels = []\n",
        "\n",
        "    num_batches = (len(test_df) - 1) // batch_size + 1\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        batch_texts = list(test_df['text'][i * batch_size: (i + 1) * batch_size])\n",
        "\n",
        "        encoded_inputs = tokenizer.batch_encode_plus(batch_texts,\n",
        "                                                     padding=True,\n",
        "                                                     truncation=True,\n",
        "                                                     max_length=508,\n",
        "                                                     return_tensors='pt')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.to(device)\n",
        "            model_outputs = model(**encoded_inputs.to(device))\n",
        "\n",
        "        batch_predicted_labels = torch.argmax(model_outputs.logits, dim=1)\n",
        "        predicted_labels.extend(batch_predicted_labels.tolist())\n",
        "\n",
        "    predicted_labels = predicted_labels[:len(test_df)]\n",
        "    return np.array(predicted_labels)"
      ],
      "metadata": {
        "id": "4Y3PK-sp3Iqy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "predictions = get_accuracy(test_df, model, tokenizer)\n",
        "\n",
        "y= np.array(test_df['label'])\n",
        "F1_score = f1_score(y, predictions)\n",
        "accuracy = accuracy_score(y, predictions)\n",
        "print(f'Testing Accuracy: {accuracy:.4f}')\n",
        "print(f'Testing F1 Score: {F1_score:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgSj4bBJ3p9f",
        "outputId": "c4183386-3b74-4efe-c876-3e2f18ccc297"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy: 0.8113\n",
            "Testing F1 Score: 0.7244\n"
          ]
        }
      ]
    }
  ]
}